{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"colab":{"name":"DataPreprocessing.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"SKPy654PfPUQ","colab_type":"text"},"source":["## **MOUNT GOOGLE DRIVE**\n"]},{"cell_type":"code","metadata":{"id":"SRcyknG8fBbx","colab_type":"code","outputId":"9a9fa028-a4be-4c24-d152-3bbe45d35e70","executionInfo":{"status":"ok","timestamp":1586322662406,"user_tz":300,"elapsed":254757,"user":{"displayName":"Conversational Traits","photoUrl":"","userId":"00452562474751385667"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BIbXPU-Ah1i-","colab_type":"text"},"source":["## **RUN THIS FOR TPU RUNTIME**"]},{"cell_type":"code","metadata":{"id":"68m2MoLRhxvv","colab_type":"code","outputId":"5aa33133-7c00-4f77-9ffc-907d3b70c3b2","executionInfo":{"status":"ok","timestamp":1586322684349,"user_tz":300,"elapsed":7523,"user":{"displayName":"Conversational Traits","photoUrl":"","userId":"00452562474751385667"}},"colab":{"base_uri":"https://localhost:8080/","height":731}},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","print(\"Tensorflow version \" + tf.__version__)\n","\n","try:\n","  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n","  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n","except ValueError:\n","  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n","\n","tf.config.experimental_connect_to_cluster(tpu)\n","tf.tpu.experimental.initialize_tpu_system(tpu)\n","tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Tensorflow version 2.2.0-rc2\n","Running on TPU  ['10.75.212.162:8470']\n","INFO:tensorflow:Initializing the TPU system: grpc://10.75.212.162:8470\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Initializing the TPU system: grpc://10.75.212.162:8470\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Clearing out eager caches\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Clearing out eager caches\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Finished initializing TPU system.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Finished initializing TPU system.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Found TPU system:\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Found TPU system:\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores: 8\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores: 8\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Workers: 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Workers: 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"yH9MkYJEfjsI","colab_type":"text"},"source":["## **DATA PREPROCESSING SCRIPT**"]},{"cell_type":"code","metadata":{"id":"WkDXUUCkZHtj","colab_type":"code","outputId":"b9ca7e50-c882-4b39-d9ef-683360b8610c","executionInfo":{"status":"ok","timestamp":1586322698045,"user_tz":300,"elapsed":7002,"user":{"displayName":"Conversational Traits","photoUrl":"","userId":"00452562474751385667"}},"colab":{"base_uri":"https://localhost:8080/","height":241}},"source":["!pip install unidecode\n","\n","import pandas as pd\n","import spacy\n","import re\n","import unidecode\n","import inflect\n","import numpy as np\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","from bs4 import BeautifulSoup\n","from IPython.display import clear_output\n","from datetime import datetime\n","import os\n","\n","#Download Stop Words If Necessary\n","nltk.download('stopwords')\n","nltk.download('words')\n","nltk.download('vader_lexicon')\n","\n","#Global Variables\n","CONTRACTION_MAP = {\n","\"ain't\": \"is not\",\"aren't\": \"are not\",\"can't\": \"cannot\",\n","\"can't've\": \"cannot have\",\"'cause\": \"because\",\"could've\": \"could have\",\n","\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\",\n","\"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\n","\"haven't\": \"have not\",\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\n","\"he'll've\": \"he he will have\",\"he's\": \"he is\",\"how'd\": \"how did\",\"how'd'y\": \"how do you\",\n","\"how'll\": \"how will\",\"how's\": \"how is\",\"I'd\": \"I would\",\"I'd've\": \"I would have\",\n","\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\n","\"i'd\": \"i would\",\"i'd've\": \"i would have\",\"i'll\": \"i will\",\"i'll've\": \"i will have\",\n","\"i'm\": \"i am\",\"i've\": \"i have\",\"isn't\": \"is not\",\"it'd\": \"it would\",\n","\"it'd've\": \"it would have\",\"it'll\": \"it will\",\"it'll've\": \"it will have\",\"it's\": \"it is\",\n","\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\n","\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n","\"mustn't've\": \"must not have\",\"needn't\": \"need not\",\"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n","\"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n","\"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\"she'll\": \"she will\",\n","\"she'll've\": \"she will have\",\"she's\": \"she is\",\"should've\": \"should have\",\"shouldn't\": \"should not\",\n","\"shouldn't've\": \"should not have\",\"so've\": \"so have\",\"so's\": \"so as\",\"that'd\": \"that would\",\n","\"that'd've\": \"that would have\",\"that's\": \"that is\",\"there'd\": \"there would\",\"there'd've\": \"there would have\",\n","\"there's\": \"there is\",\"they'd\": \"they would\",\"they'd've\": \"they would have\",\"they'll\": \"they will\",\n","\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\"to've\": \"to have\",\n","\"wasn't\": \"was not\",\"we'd\": \"we would\",\"we'd've\": \"we would have\",\"we'll\": \"we will\",\n","\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\"weren't\": \"were not\",\n","\"what'll\": \"what will\",\"what'll've\": \"what will have\",\"what're\": \"what are\",\"what's\": \"what is\",\n","\"what've\": \"what have\",\"when's\": \"when is\",\"when've\": \"when have\",\"where'd\": \"where did\",\n","\"where's\": \"where is\",\"where've\": \"where have\",\"who'll\": \"who will\",\"who'll've\": \"who will have\",\n","\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\"why've\": \"why have\",\n","\"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\"would've\": \"would have\",\n","\"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\",\"y'all\": \"you all\",\"y'all'd\": \"you all would\",\n","\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\",\n","\"you'd've\": \"you would have\",\"you'll\": \"you will\",\"you'll've\": \"you will have\",\"you're\": \"you are\",\"you've\": \"you have\"\n","}\n","\n","#Slang Map For Words With Slang\n","with open('/content/drive/My Drive/Code/slang.txt') as file:\n","        slang_map = dict(map(str.strip, line.partition('\\t')[::2]) for line in file if line.strip())\n","\n","#Inflection Engine For Number To Word Conversion\n","inflectEngine = inflect.engine()\n","\n","#Custom Functions\n","\n","#Remove \"RT\" tag\n","def removeRT(text):\n","    text = text.replace(\"RT\", \"\", 1)\n","    return text\n","    \n","#Removing HTML Tags\n","def stripHtmlTags(text):\n","    soup = BeautifulSoup(text, \"html.parser\")\n","    stripped_text = soup.get_text()\n","    return stripped_text\n","\n","#Remove URL\n","def removeURL(text):\n","    text = re.sub(r\"http\\S+\", \"\", text)\n","    return text\n","\n","#Remove Non English Words\n","def removeNonEnglish(text):\n","    words = set(nltk.corpus.words.words())\n","    text = \" \".join(w for w in nltk.wordpunct_tokenize(text) if w.lower() in words or not w.isalpha())\n","    return text\n","\n","#Contraction Expansion\n","def expandContractions(text, contraction_mapping = CONTRACTION_MAP):\n","    \n","    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n","                                      flags=re.IGNORECASE|re.DOTALL)\n","    def expand_match(contraction):\n","        match = contraction.group(0)\n","        first_char = match[0]\n","        expanded_contraction = contraction_mapping.get(match)\\\n","                                if contraction_mapping.get(match)\\\n","                                else contraction_mapping.get(match.lower())                       \n","        expanded_contraction = first_char+expanded_contraction[1:]\n","        return expanded_contraction\n","        \n","    expanded_text = contractions_pattern.sub(expand_match, text)\n","    expanded_text = re.sub(\"'\", \"\", expanded_text)\n","    return expanded_text\n","\n","#Removing Accented Characters\n","def removeAccentedChars(text):\n","    text = unidecode.unidecode(text)\n","    return text\n","\n","#Remove User Mentions\n","def removeUserMentions(text):\n","    text = re.sub(r\"(?:\\@)\\S+\", \"\", text)\n","    return text\n","\n","#Convert Text To Lower Case\n","def lowerCaseConversion(text):\n","    text = \" \".join(x.lower() for x in str(text).split())\n","    return text\n","\n","#Remove special characters\n","def specialCharsRemoval(text, remove_digits=False):\n","    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n","    text = re.sub(pattern, '', text)\n","    return text\n","\n","#Remove Emoticons\n","def removeEmoticons(text):\n","    text = re.sub(':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=/|:/|X\\-\\(|>\\.<|>=\\(|D:', '', text)\n","    return text\n","\n","#Remove Stop Words\n","def stopWordsRemoval(text):\n","    stop = stopwords.words('english')\n","    stop.extend(['come','order','try','go','get','make','drink','plate','dish','restaurant','place',\n","                 'would','really','like','great','service','came','got'])\n","    text = \" \".join([x for x in text.split() if x not in stop])\n","    return text\n","\n","#Convert Numbers To Words\n","def numberToWords(text):  \n","    temp_str = text.split() \n","    new_string = [] \n","  \n","    for word in temp_str: \n","        if word.isdigit(): \n","            temp = inflectEngine.number_to_words(word) \n","            new_string.append(temp)\n","        else: \n","            new_string.append(word) \n","  \n","    text = ' '.join(new_string) \n","    return text\n","\n","#Remove Numbers From Text\n","def removeNumbers(text):\n","    for word in text.split():\n","        if word.isnumeric():\n","            text = text.replace(word, '')\n","    return text\n","\n","#Remove Adverbs\n","def removeAdverbs(text):\n","    adverbs = ['accidentally', 'always', 'angrily', 'anxiously', 'awkwardly', 'badly', 'blindly', 'boastfully', 'boldly', 'bravely', 'brightly', 'cheerfully', 'coyly', 'crazily', 'defiantly', 'deftly', 'deliberately', 'devotedly', 'doubtfully', 'dramatically', 'dutifully', 'eagerly', 'elegantly', 'enormously', 'evenly', 'eventually', 'exactly', 'faithfully', 'finally', 'foolishly', 'fortunately', 'frequently', 'gleefully', 'gracefully', 'happily', 'hastily', 'honestly', 'hopelessly', 'hourly', 'hungrily', 'innocently', 'inquisitively', 'irritably', 'jealously', 'justly', 'kindly', 'lazily', 'loosely', 'madly', 'merrily', 'mortally', 'mysteriously', 'nervously', 'never', 'obediently', 'obnoxiously', 'occasionally', 'often', 'only', 'perfectly', 'politely', 'poorly', 'powerfully', 'promptly', 'quickly', 'rapidly', 'rarely', 'regularly', 'rudely', 'safely', 'seldom', 'selfishly', 'seriously', 'shakily', 'sharply', 'silently', 'slowly', 'solemnly', 'sometimes', 'speedily', 'sternly', 'technically', 'tediously', 'unexpectedly', 'usually', 'victoriously', 'vivaciously', 'warmly', 'wearily', 'weekly', 'wildly', 'yearly', 'not', 'also', 'very', 'often', 'however', 'too', 'usually', 'really', 'early', 'never', 'always', 'sometimes', 'together', 'likely', 'simply', 'generally', 'instead', 'actually', 'again', 'rather', 'almost', 'especially', 'ever', 'quickly', 'probably', 'already', 'below', 'directly', 'therefore', 'else', 'thus', 'easily', 'eventually', 'exactly', 'certainly', 'normally', 'currently', 'extremely', 'finally', 'constantly', 'properly', 'soon', 'specifically', 'ahead', 'daily', 'highly', 'immediately', 'relatively', 'slowly', 'fairly', 'primarily', 'completely', 'ultimately', 'widely', 'recently', 'seriously', 'frequently', 'fully', 'mostly', 'naturally', 'nearly', 'occasionally', 'carefully', 'clearly', 'essentially', 'possibly', 'slightly', 'somewhat', 'equally', 'greatly', 'necessarily', 'personally', 'rarely', 'regularly', 'similarly', 'basically', 'closely', 'effectively', 'initially', 'literally', 'mainly', 'merely', 'gently', 'hopefully', 'originally', 'roughly', 'significantly', 'totally', 'twice', 'elsewhere', 'everywhere', 'obviously', 'perfectly', 'physically', 'successfully', 'suddenly', 'truly', 'virtually', 'altogether', 'anyway', 'automatically', 'deeply', 'definitely', 'deliberately', 'hardly', 'readily', 'terribly', 'unfortunately', 'forth', 'briefly', 'moreover', 'strongly', 'honestly', 'previously', 'as', 'there', 'when', 'how', 'so', 'up', 'out', 'no', 'only', 'well', 'then', 'first', 'where', 'why', 'now', 'around', 'once', 'down', 'off', 'here', 'tonight', 'away', 'today', 'far', 'quite', 'later', 'above', 'yet', 'maybe', 'otherwise', 'near', 'forward', 'somewhere', 'anywhere', 'please', 'forever', 'somehow', 'absolutely', 'abroad', 'yeah', 'nowhere', 'tomorrow', 'yesterday', 'the', 'to', 'in', 'on', 'by', 'more', 'about', 'such', 'through', 'new', 'just', 'any', 'each', 'much', 'before', 'between', 'free', 'right', 'best', 'since', 'both', 'sure', 'without', 'back', 'better', 'enough', 'lot', 'small', 'though', 'less', 'little', 'under', 'next', 'hard', 'real', 'left', 'least', 'short', 'last', 'within', 'along', 'lower', 'TRUE', 'bad', 'across', 'clear', 'easy', 'full', 'close', 'late', 'proper', 'fast', 'wide', 'item', 'wrong', 'ago', 'behind', 'quick', 'straight', 'direct', 'extra', 'morning', 'pretty', 'overall', 'alone', 'bright', 'flat', 'whatever', 'slow', 'clean', 'fresh', 'whenever', 'cheap', 'thin', 'cool', 'fair', 'fine', 'smooth', 'FALSE', 'thick', 'collect', 'nearby', 'wild', 'apart', 'none', 'strange', 'tourist', 'aside', 'loud', 'super', 'tight', 'gross', 'ill', 'downtown', 'honest', 'ok', 'pray', 'weekly']\n","    text = \" \".join([x for x in text.split() if x not in stop])\n","    return text\n","\n","#Remove Slang Words\n","def removeSlangWords(text):\n","    words = text.split()\n","    for word in words:\n","        if word in slang_map.keys():\n","            text = text.replace(word, slang_map[word])\n","    return text\n","\n","#Remove Whitespace From Text \n","def removeWhitespace(text): \n","    return  \" \".join(text.split())\n","\n","#Main Data Preprocessing Method\n","def dataPreprocessing(corpus, RTremoval = False, userMentionRemoval = False, toLowerCase = False, stripHTML = False, URLRemoval = False, \n","                      expandContraction = False, slangWordsRemoval = False, stripAccentedChars = False, NonEnglishRemoval = False, \n","                      removeSplChars = False, removeStopWords = False, adverbsRemoval = False, numbersRemoval = False, numToWords = False, \n","                      whiteSpaceRemoval = False):\n","    for i in range(len(corpus)):\n","        clear_output(wait=True)\n","        print((\"%d of %d tweets processed...\")%(i+1, len(corpus)))\n","        if RTremoval:\n","            corpus[i] = removeRT(corpus[i])\n","        if userMentionRemoval:\n","            corpus[i] = removeUserMentions(corpus[i])\n","        if toLowerCase:\n","            corpus[i] = lowerCaseConversion(corpus[i])\n","        if stripHTML:\n","            corpus[i] = stripHtmlTags(corpus[i])\n","        if URLRemoval:\n","            corpus[i] = removeURL(corpus[i])\n","        if expandContraction:\n","            corpus[i] = expandContractions(corpus[i])\n","        if slangWordsRemoval:\n","            corpus[i] = removeSlangWords(corpus[i])\n","        if stripAccentedChars:\n","            corpus[i] = removeAccentedChars(corpus[i])\n","        if NonEnglishRemoval:\n","            corpus[i] = removeNonEnglish(corpus[i])\n","        if removeSplChars:\n","            corpus[i] = specialCharsRemoval(corpus[i])\n","        if removeStopWords:\n","            corpus[i] = stopWordsRemoval(corpus[i])\n","        if adverbsRemoval:\n","            corpus[i] = removeAdverbs(corpus[i])\n","        if numbersRemoval:\n","            corpus[i] = removeNumbers(corpus[i])\n","        if numToWords:\n","            corpus[i] = numberToWords(corpus[i])\n","        if whiteSpaceRemoval:\n","            corpus[i] = removeWhitespace(corpus[i])\n","    return(corpus)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting unidecode\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n","\r\u001b[K     |█▍                              | 10kB 18.5MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 61kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81kB 3.1MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 92kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 102kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 112kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 122kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 133kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 143kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 153kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 163kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 174kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 184kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 194kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 204kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 215kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 225kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 235kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 3.5MB/s \n","\u001b[?25hInstalling collected packages: unidecode\n","Successfully installed unidecode-1.1.1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n","  warnings.warn(\"The twython library has not been installed. \"\n"],"name":"stderr"},{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/words.zip.\n","[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"74-XNNhWSc9V","colab_type":"text"},"source":["## **DATA ACCUMULATION**"]},{"cell_type":"code","metadata":{"id":"ZaxC3QMBhTQ5","colab_type":"code","outputId":"d6f0fc37-08fe-4775-e7cb-41b8c311fef4","executionInfo":{"status":"ok","timestamp":1585039849790,"user_tz":300,"elapsed":21,"user":{"displayName":"Conversational Traits","photoUrl":"","userId":"00452562474751385667"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["#PathList = [\"/content/drive/My Drive/Data/Twitter/Politics/\", \"/content/drive/My Drive/Data/Twitter/Corona/\", \"/content/drive/My Drive/Data/Twitter/Sports/\"]\n","PathList = [\"/content/drive/My Drive/Data/Twitter/Sports/\"]\n","titleList = ['Politics', 'Corona', 'Sports']\n","titleFlag = 2\n","for path in PathList:\n","  dataList = os.listdir(path)\n","  \n","  #Declare final data list\n","  finalData = []\n","  print(dataList)\n","  for data in dataList:\n","    print(path+data)\n","    dataDf = pd.read_csv(path+data)\n","\n","    #Required Columns from the stream\n","    cols = ['created_at', 'text', 'retweeted_status_text', 'extended_tweet_full_text', 'retweeted_status_extended_tweet_full_text']\n","\n","    #Drop unwanted columns from the dataframe\n","    allCols = list(dataDf.columns)\n","    print(\"Creating a dataframe with only required columns from the streamed data... Task Starting Time: \" + datetime.now().strftime(\"%m/%d/%y %H:%M\"))\n","    dropCols = []\n","    for colName in allCols:\n","        if colName not in cols:\n","            dropCols.append(colName)\n","\n","    dataDf.drop(dropCols, axis=1, inplace=True)\n","    print(\"Task Ending Time: \" + datetime.now().strftime(\"%m/%d/%y %H:%M\"))\n","\n","    #Create a list of all text content and time\n","    df_len = len(dataDf)\n","    for i in range(df_len):\n","      clear_output(wait=True)\n","      print((\"Directory: %s\\nFile: %s \\n%d of %d tweets processed...\")%(path, data, i+1, df_len))\n","\n","      #Create Time list\n","      #finalTime.append(dataDf['created_at'][i])\n","\n","      #Create Text List\n","      if str(dataDf['extended_tweet_full_text'][i]) != 'nan':\n","        finalData.append([dataDf['created_at'][i], dataDf['extended_tweet_full_text'][i]])\n","      elif str(dataDf['retweeted_status_extended_tweet_full_text'][i]) != 'nan':\n","        finalData.append([dataDf['created_at'][i], dataDf['retweeted_status_extended_tweet_full_text'][i]])\n","      else:\n","        finalData.append([dataDf['created_at'][i], dataDf['text'][i]])\n","\n","  print(len(finalData))\n","  \n","  #Create full dataset\n","  allData = pd.DataFrame(data = finalData, columns = ['time', 'tweet'])\n","  allData.to_csv('/content/drive/My Drive/Data/Twitter/'+titleList[titleFlag]+'.csv')\n","  titleFlag = titleFlag + 1"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Directory: /content/drive/My Drive/Data/Twitter/Sports/\n","File: TwitterData_Sports_03_17_20__10_56 - TwitterData_Sports_03_17_20__10_56.csv \n","599 of 599 tweets processed...\n","383890\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eNl571IoVRWb","colab_type":"code","outputId":"b775a478-04d3-412c-9a9c-9cdcc3de2efe","executionInfo":{"status":"ok","timestamp":1585185927100,"user_tz":300,"elapsed":348,"user":{"displayName":"Conversational Traits","photoUrl":"","userId":"00452562474751385667"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(\"hello\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["hello\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4uA9ke1-Sihk","colab_type":"text"},"source":["## **DATA PREPROCESSING**"]},{"cell_type":"code","metadata":{"id":"0o7u7Xnwm4WQ","colab_type":"code","outputId":"24950568-d444-4166-86e9-5ee01c428f4d","executionInfo":{"status":"error","timestamp":1585086168972,"user_tz":300,"elapsed":1082,"user":{"displayName":"Conversational Traits","photoUrl":"","userId":"00452562474751385667"}},"colab":{"base_uri":"https://localhost:8080/","height":374}},"source":["dataDir = \"/content/drive/My Drive/Data/Twitter/\"\n","titleList = ['Politics', 'Sports']\n","for title in titleList:\n","  dataDf = pd.read_csv(dataDir+title+'.csv')\n","  dataDf['tweet'] = dataPreprocessing(dataDf['tweet'], RTremoval = True, userMentionRemoval = True, toLowerCase = True, stripHTML = True, \n","                                      URLRemoval = True, expandContraction = True, whiteSpaceRemoval = True)\n","  dataDf.to_csv(dataDir+title+'.csv')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["92046 of 1250561 tweets processed...\n"],"name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-6cdbc5c5c200>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mdataDf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataDir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   dataDf['tweet'] = dataPreprocessing(dataDf['tweet'], RTremoval = True, userMentionRemoval = True, toLowerCase = True, stripHTML = True, \n\u001b[0;32m----> 6\u001b[0;31m                                       URLRemoval = True, expandContraction = True, whiteSpaceRemoval = True)\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mdataDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataDir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-c857a8d29954>\u001b[0m in \u001b[0;36mdataPreprocessing\u001b[0;34m(corpus, RTremoval, userMentionRemoval, toLowerCase, stripHTML, URLRemoval, expandContraction, slangWordsRemoval, stripAccentedChars, NonEnglishRemoval, removeSplChars, removeStopWords, adverbsRemoval, numbersRemoval, numToWords, whiteSpaceRemoval)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%d of %d tweets processed...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mRTremoval\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremoveRT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muserMentionRemoval\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremoveUserMentions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-c857a8d29954>\u001b[0m in \u001b[0;36mremoveRT\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m#Remove \"RT\" tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremoveRT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'replace'"]}]}]}